{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\evanm\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from deployment.speech_inference import SpeechTranscriptionPipeline, ModelOptimization\n",
    "from time import time\n",
    "\n",
    "model_name = \"evanmazor/whisper-medium-finetuned\"   # e.g., \"KevinKibe/whisper-small-af\"\n",
    "task = \"translate\"\n",
    "# task = \"translate\"\n",
    "audiofile_dir = \"afrikaans_sample2.wav\"                      # filetype should be .mp3 or .wav\n",
    "\n",
    "# Optimize model for better results\n",
    "model_optimizer = ModelOptimization(model_name=model_name)\n",
    "# model_optimizer.convert_model_to_optimized_format()\n",
    "model = model_optimizer.load_transcription_model(task, is_v3_architecture=False, language='af')\n",
    "# For fine-tuning v3 or v3-turbo models or a fine-tuned version of them, specify is_v3_architecture=True\n",
    "# Example:\n",
    "# model = model_optimizer.load_transcription_model(is_v3_architecture=True)\n",
    "\n",
    "# Optional language parameter, else model will automatically detect language.\n",
    "# Example:\n",
    "# model = model_optimizer.load_transcription_model(language='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline mode - skipping HF login\n",
      "Suppressing numeral and symbol tokens\n",
      "Progress: 25.00%...\n",
      "Progress: 50.00%...\n",
      "Progress: 75.00%...\n",
      "Progress: 100.00%...\n",
      "Transcription took 44.28109383583069 seconds\n",
      "{'text': ' Out of the blue of our sky, out of the depths of our seas, over our eternal mountains, where the cranes answer you, through our far-abandoned plains, with the creaking of our waves, rises the voice of our loved ones, from our land South Africa. We shall answer your call, we shall sacrifice what you ask, we shall live, we shall die, we are for you South Africa. In the midst of our prayers,', 'start': 0.2, 'end': 28.06}\n",
      "{'text': ' in our heart and soul and spirit, in our rumour of our past, in our hope of what will be, in our will and work and walk, from our path to our grave, do not let any other country take our love, do not let any other faith take us away. Fatherland, we will honour the dignity of your name, true and true as Africans, children of South Africa. In the sunshine of our summer', 'start': 28.668, 'end': 56.967}\n",
      "{'text': \" in our winter night so cold, in the lint of our love, in the lamp of our love, at the sound of heaven's bells, at the sound of the knock on the pillow, spread your voice, we will never forget you, you know where your children are. On your call we will never say no, we will always say yes, to live and to die, yes, we are coming South Africa, because you all must firmly believe in our fatherly building,\", 'start': 57.355, 'end': 85.418}\n",
      "{'text': ' may we also have the strength, O Lord, to keep our hands off and to keep the heritage of our fathers for the heritage of our children, blessed by the Most High, who are free in the whole world. As our fathers trusted, let us also trust, O Lord. With our land and with our nation, it will be well, God the Lord.', 'start': 86.347, 'end': 107.592}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initiate the transcription model\n",
    "\n",
    "# Monkey patch the login method to skip the login\n",
    "SpeechTranscriptionPipeline._login_to_huggingface = lambda self: print(\"Offline mode - skipping HF login\")\n",
    "\n",
    "t = time()\n",
    "inference = SpeechTranscriptionPipeline(\n",
    "    audio_file_path=audiofile_dir,\n",
    "    task=task,\n",
    "    huggingface_token='dummy'  # Don't need it cuz monkey patch\n",
    ")\n",
    "\n",
    "# To get transcriptions\n",
    "transcription = inference.transcribe_audio(model=model)\n",
    "print('Transcription took', time()-t, 'seconds')\n",
    "for segment in transcription['segments']:\n",
    "    print(segment)\n",
    "\n",
    "# To get transcriptions with speaker labels\n",
    "# t = time()\n",
    "# transcription['language'] = 'en'\n",
    "# alignment_result = inference.align_transcription(transcription) # Optional parameter alignment_model: if the default wav2vec alignment model is not available e.g thinkKenya/wav2vec2-large-xls-r-300m-sw\n",
    "# print(alignment_result, 'took', time()-t, 'seconds')\n",
    "\n",
    "# t = time()\n",
    "# diarization_result = inference.diarize_audio(alignment_result)\n",
    "# print(diarization_result, 'took', time()-t, 'seconds')\n",
    "\n",
    "# #To generate subtitles(.srt format), will be saved in root directory\n",
    "# inference.generate_subtitles(transcription, alignment_result, diarization_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
